{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a9920-5590-4200-824e-6d685a99b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import preprocessing as pp\n",
    "import machine_learning as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee74dce-cdcb-4be0-8437-4eaab1a7d590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(hp):\n",
    "    \"\"\"\n",
    "    This method returns a TensorFlow model. It is a modified duplicate of the one from machine_learning.\n",
    "    \"\"\"\n",
    "    input_shape = 500\n",
    "    num_classes = 4\n",
    "    \n",
    "    filter1 = hp.Int('filter1', 3, 128, step=2)\n",
    "    filter2 = hp.Int('filter2', 3, 128, step=1)\n",
    "    filter3 = hp.Int('filter3', 3, 128, step=1)\n",
    "    filtersize1 = hp.Int('filtersize1', 1, 15, step=2)\n",
    "    filtersize2 = hp.Int('filtersize2', 1, 15, step=2)\n",
    "    filtersize3 = hp.Int('filtersize3', 1, 4, step=1)\n",
    "    poolsize1 = hp.Int('poolsize1', 1, 4, step=1)\n",
    "    poolsize2 = hp.Int('poolsize2', 1, 4, step=1)\n",
    "    poolsize3 = hp.Int('poolsize3', 1, 4, step=1)\n",
    "    gaussian_noise = hp.Float('noise', 0.0, 0.3, step=0.05, default=0.1)\n",
    "    dropout = hp.Float('dropout', 0.0, 0.5, step=0.1, default=0.2)\n",
    "    dense_size = hp.Int('dense', 32, 1024, step=16)\n",
    "    \n",
    "    dilation_1 = hp.Int('dilation_1', 1, 3, step=1, default=1)\n",
    "    dilation_2 = hp.Int('dilation_2', 1, 3, step=1, default=2)\n",
    "    dilation_3 = hp.Int('dilation_3', 1, 3, step=1, default=3)\n",
    "    \n",
    "    activation = 'gelu'\n",
    "    padding='causal'\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Reshape((input_shape, 1)),\n",
    "        \n",
    "        layers.GaussianNoise(gaussian_noise),\n",
    "\n",
    "        layers.Conv1D(filter1, filtersize1, activation=activation, padding=padding, dilation_rate=dilation_1),\n",
    "        #layers.Conv1D(8, hp.Choice('filter1size',values=[2,4,8]), activation=activation),\n",
    "        layers.MaxPooling1D(pool_size=poolsize1),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        #layers.Conv1D(hp.Choice('filter2',values=[16,32,64]), hp.Choice('filter2size',values=[1,2,3]), activation=activation),\n",
    "        #layers.Conv1D(16,3, activation=activation),\n",
    "        layers.Conv1D(filter2, filtersize2, activation=activation, padding=padding, dilation_rate=dilation_2),\n",
    "        layers.MaxPooling1D(pool_size=poolsize2),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv1D(filter3, filtersize3, activation=activation, padding=padding, dilation_rate=dilation_3),\n",
    "        layers.MaxPooling1D(pool_size=poolsize3),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.SpatialDropout1D(dropout),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_size, activation=activation),\n",
    "\n",
    "        layers.Dense(num_classes),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089eb97b-4ab7-4b98-9454-fc1233287b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "use_new_data=False\n",
    "num_test_data_sets = 5\n",
    "\n",
    "data_directory = os.path.join(\"..\", \"DownloadedData\", \"Rev230meas2.5cm/\")\n",
    "\n",
    "offset_header_rev1 = -200\n",
    "offset_header_rev2 = 200\n",
    "if \"rev1\" in data_directory.lower():\n",
    "    offset_header = offset_header_rev1\n",
    "elif \"rev2\" in data_directory.lower():\n",
    "    offset_header = offset_header_rev2\n",
    "else:\n",
    "    raise NotImplementedError(\"Specify which offset should be used!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb7930-54d1-4e34-8d58-c03ace7beb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical details of your experimental setup\n",
    "technical_details = {\n",
    "    \"header_length\": 1,  # Number of symbols used to identify the header\n",
    "    \"clock_freq\": int(100e6),  # Clock frequency (Hz) of the sender electronics\n",
    "    \"sample_freq\": int(1e10),  # Sample frequency (Hz) used by oscilloscope\n",
    "    \"signal_length\": 2_000_002,  # Total number of data points of one measurement\n",
    "    \"steps_to_left\": 2,  # start for the cut position in symbols to the left from the header position\n",
    "    \"steps_to_right\": 3,  # end for the cut position in symbols to the right from the header position\n",
    "    \"do_normalize_data\": True,  # Use if data should be normalized to have zero mean and std 1\n",
    "}\n",
    "\n",
    "# Array of the different distances in cm (folder names) used for the measurement\n",
    "# position = [dirname for dirname in os.listdir(data_directory) \n",
    "            # if os.path.isdir(os.path.join(data_directory,dirname))]\n",
    "position = [dirname for dirname in os.listdir(data_directory) \n",
    "                if os.path.isdir(os.path.join(data_directory,dirname))]\n",
    "\n",
    "print(f\"Evaluating datasets: {position}\")\n",
    "\n",
    "actual_dataset_used = position[0]\n",
    "print(f\"{actual_dataset_used=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cba52-820e-4058-a241-93c19f3ae772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    print(data.dtype)\n",
    "    return np.eye(nb_classes)[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af969409-2b19-4c66-a5c6-55badf735280",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels, validation_data, validation_labels = pp.get_datasets(\n",
    "    os.path.join(data_directory, actual_dataset_used), \n",
    "    technical_details=technical_details,\n",
    "    offset_header=offset_header,\n",
    "    data_augmentation_halflength=1, \n",
    "    debug=debug, \n",
    "    force_create_npy_files=use_new_data\n",
    ")\n",
    "\n",
    "print(training_data.shape)\n",
    "print(training_labels.shape)\n",
    "resnet = False\n",
    "if resnet:\n",
    "    # reshape to artificially make 2d data, so it can be processed by the HyperResNet\n",
    "    training_data = training_data.reshape((-1,500,1,1))\n",
    "    validation_data = validation_data.reshape((-1,500,1,1))\n",
    "\n",
    "train_ds = ml.prepare_datasets_nosplit(training_data, training_labels, batch_size=128)\n",
    "val_ds = ml.prepare_datasets_nosplit(validation_data, validation_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20a227-a4fc-4745-9df5-a5a5ab640d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner = kt.RandomSearch(\n",
    "#tuner = kt.BayesianOptimization(\n",
    "# kt.applications.HyperXception(input_shape=(500,1,1), classes=4)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "     kt.applications.HyperXception(input_shape=(500,1), classes=4) if resnet else get_model,\n",
    "     objective='val_accuracy',\n",
    "     max_epochs=10,\n",
    "     hyperband_iterations=3,\n",
    "     #max_trials=20,\n",
    "     project_name=f'hyperparam_tuning_rev2_fixed_dilation',\n",
    "     seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2e9ca-1749-4d12-bb39-573ae2157753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    halt_callback = ml.HaltThresholdCallback(metric='val_accuracy', threshold=.99)\n",
    "\n",
    "    tuner.search(train_ds, validation_data=val_ds, \n",
    "                 shuffle=True,\n",
    "                # use_multiprocessing=True,\n",
    "                 #workers=6,\n",
    "                 epochs=10,\n",
    "                 callbacks=[\n",
    "                     tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "                     # halt_callback\n",
    "                    ],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca9e2d-ef5a-45db-ad7a-83d676246a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "for j in range(0,num_test_data_sets):\n",
    "    test_data, test_labels, test_start = pp.load_test_datasets(\n",
    "                os.path.join(data_directory, actual_dataset_used), technical_details,\n",
    "                data_index=j, offset_header=offset_header,\n",
    "            )\n",
    "\n",
    "    print(ml.test_model(best_model, test_data, test_labels, test_start, technical_details))\n",
    "    \n",
    "    if resnet:\n",
    "        # reshape to artificially make 2d data, so it can be processed by the HyperResNet\n",
    "        processed_test_data = processed_test_data.reshape((-1,500,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d23fc-dcf8-42b9-99ef-6fc7d775c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.test_model_all_displacements(best_model, test_data, test_labels, test_start, technical_details=technical_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371b782-df29-4269-98e4-4ff5addc620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "print(best_hyperparameters.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
