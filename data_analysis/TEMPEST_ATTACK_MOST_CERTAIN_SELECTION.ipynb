{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e945818b-8509-4b61-bc5f-cf8b1606e0b2",
   "metadata": {},
   "source": [
    "Trains a neural network on a given dataset and saves test accuracy into a json file. Examine effect of selecting the network's \"most confident\" predictions and only looking at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f60de-7398-49a9-a734-3f19e817e32a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import machine_learning as ml\n",
    "import preprocessing as pp\n",
    "from verify_dataset_hash import verify_json_metadata, get_repo_sha1_and_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab874a3-e906-459b-b847-d85897ef6697",
   "metadata": {},
   "source": [
    "data_directory: directory containing measured data (train and test datasets)\n",
    "\n",
    "debug: Set True if you want to see the header detection result during the pre processing\n",
    "\n",
    "use_new_data: Set True if data needs to be loaded from a txt file for the first time (saves time otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320dcf4-a857-47e0-8de6-99a0ce4eb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory=os.path.join(\"..\", \"DownloadedData\", \"Rev1Distance\")\n",
    "\n",
    "debug=False\n",
    "use_new_data=False\n",
    "num_test_data_sets = 3\n",
    "\n",
    "offset_header_rev1 = -200\n",
    "offset_header_rev2 = 200\n",
    "if \"rev1\" in data_directory.lower():\n",
    "    offset_header = offset_header_rev1\n",
    "elif \"rev2\" in data_directory.lower():\n",
    "    offset_header = offset_header_rev2\n",
    "else:\n",
    "    raise NotImplementedError(\"Specify which offset should be used!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91866f75-1bcb-4089-839f-5b80ca993610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical details of your experimental setup\n",
    "technical_details = {\n",
    "    \"header_length\": 1,  # Number of symbols used to identify the header\n",
    "    \"clock_freq\": int(100e6),  # Clock frequency (Hz) of the sender electronics\n",
    "    \"sample_freq\": int(1e10),  # Sample frequency (Hz) used by oscilloscope\n",
    "    \"signal_length\": 2_000_002,  # Total number of data points of one measurement\n",
    "    \"steps_to_left\": 2,  # start for the cut position in symbols to the left from the header position\n",
    "    \"steps_to_right\": 3,  # end for the cut position in symbols to the right from the header position\n",
    "    \"do_normalize_data\": True,  # Use if data should be normalized to have zero mean and std 1\n",
    "}\n",
    "\n",
    "# Array of the different distances in cm (folder names) used for the measurement\n",
    "positions = [dirname for dirname in os.listdir(data_directory) \n",
    "                if os.path.isdir(os.path.join(data_directory,dirname))]\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "positions.sort(key=natural_keys)\n",
    "\n",
    "train_accuracy = {} # Tracks the resulting train accuracy\n",
    "validation_accuracy = {}  # Tracks the resulting validation accuracy\n",
    "test_accuracy = {}  # Tracks the resulting test accuracy\n",
    "\n",
    "machine_learning_settings = {\n",
    "    \"dropout\": 0.1,\n",
    "    \"training_EPOCHS\": 4,\n",
    "}\n",
    "\n",
    "print(positions)\n",
    "\n",
    "data_augmentation_halflength = 1\n",
    "\n",
    "stop_at_val_acc = 0.99\n",
    "\n",
    "label_mapping = pp.translate_all  # lambda labels: pp.translate_all_onlybit(labels, '0101') #\n",
    "num_classes = 4 if label_mapping == pp.translate_all else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4a0d8-97e0-4ac7-b635-b1209cef0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_positions(positions, technical_details, num_test_data_sets, debug, num_classes):\n",
    "    print(f\"Evaluating datasets: {positions}\")\n",
    "    \n",
    "    for i, elem in enumerate(positions):\n",
    "        print(f\"Loading dataset {i}: {elem}\")\n",
    "        training_data, training_labels, validation_data, validation_labels = pp.get_datasets(\n",
    "            os.path.join(data_directory, elem), technical_details, offset_header=offset_header,\n",
    "            data_augmentation_halflength=data_augmentation_halflength, debug=debug, force_create_npy_files=use_new_data\n",
    "        )\n",
    "        \n",
    "        model = ml.get_model_hyperparam_improved(technical_details, num_classes)\n",
    "        # model = ml.create_model_residual(technical_details)\n",
    "\n",
    "        hist = ml.train_model(model,\n",
    "                       training_data, training_labels,\n",
    "                       machine_learning_settings,\n",
    "                       validation_data=validation_data,\n",
    "                       validation_labels=validation_labels,\n",
    "                       training_run_name=str(positions),\n",
    "                       early_stopping_patience=4,\n",
    "                       label_mapping=label_mapping,\n",
    "                       stop_at_val_acc=stop_at_val_acc,\n",
    "                      )\n",
    "\n",
    "        list_acc = []\n",
    "        train_accuracy[f\"{elem}\"] = hist.history[\"accuracy\"][-1]\n",
    "        validation_accuracy[f\"{elem}\"] = hist.history[\"val_accuracy\"][-1]\n",
    "        for j in range(0,num_test_data_sets):\n",
    "            test_data, test_labels, test_start = pp.load_test_datasets(\n",
    "                os.path.join(data_directory, elem), technical_details,\n",
    "                data_index=j, offset_header=offset_header,\n",
    "            )\n",
    "            list_acc.append(\n",
    "                ml.test_model(model, test_data, test_labels, \n",
    "                              test_start, technical_details, label_mapping=label_mapping,\n",
    "                             most_certain_fraction=0.01,)\n",
    "            )\n",
    "\n",
    "        test_accuracy[f\"{elem}\"] = list_acc\n",
    "        print(\"\\n\")\n",
    "        pp.write_result_dict({\"test_accuracies\": test_accuracy, \"train_accuracies\": train_accuracy, \n",
    "                      \"validation_accuracies\": validation_accuracy}, target_file_path=result_file_path, override=True, )\n",
    "        \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc5936-8054-40e6-9397-6597ba56bcb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "git_sha1, git_diff = get_repo_sha1_and_diff(search_parent_directories=True)\n",
    "result_file_path = os.path.join(data_directory, f\"result_TEMPEST_{datetime.datetime.now().strftime('%Y%m%d-%H%M')}.json\")\n",
    "try:\n",
    "    measurement_metadata = verify_json_metadata(data_directory, verbose=False)\n",
    "except:\n",
    "    measurement_metadata = \"FAILURE_TO_GET_METADATA\"\n",
    "\n",
    "test_accuracy = evaluate_all_positions(positions, technical_details, num_test_data_sets, debug, num_classes)\n",
    "pp.write_result_dict({\n",
    "    'test_accuracies': test_accuracy, \n",
    "    'train_accuracies': train_accuracy, \n",
    "    'validation_accuracies': validation_accuracy,\n",
    "    'measurement_metadata': measurement_metadata, \n",
    "    'n_classes': num_classes,\n",
    "    'data_augmentation_halflength': data_augmentation_halflength,\n",
    "    }, \n",
    "    target_file_path=result_file_path, override=True, )\n",
    "\n",
    "print(\" ------------ Done ---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928c4c3-415c-4d79-bbdc-75b289eb3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b5e76-60a7-4ac2-9908-89847c085a06",
   "metadata": {},
   "source": [
    "# ------------------ Metadata ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93639fe-5cdf-49a4-88f3-109ce37453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repository git information (WARNING: THIS IS ONLY RELIABLE IF NO CHANGES WERE MADE BETWEEN RUNNING DIFFERENT NOTEBOOK CELLS!)\n",
    "print(git_sha1)\n",
    "print(\"\\nFull git diff (only Python source files!):\\n\")\n",
    "[print(d) for d in git_diff if d.a_path.endswith(\".py\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
